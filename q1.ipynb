{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramyanee/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "string.punctuation\n",
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import*\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-8d40498fd0f1>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('yelp_labelled.txt',sep=\".\\t\",header=None)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('yelp_labelled.txt',sep=\".\\t\",header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing \n",
    "\n",
    "### Steps: \n",
    "#### removing punctuations  \n",
    "#### removing urls  \n",
    "#### removing htms tags if any  \n",
    "#### removing stopwords  \n",
    "#### converting to lower case \n",
    "#### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(re.compile('<.*?>') , '', text)\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = text.lower()\n",
    "    text = \"\".join([t for t in text if t not in string.punctuation])\n",
    "    text = remove_stopwords(text)\n",
    "    text = word_tokenize(text)\n",
    "    return np.array(text)\n",
    "\n",
    "def avgw2v(text,dim,model):\n",
    "    n = len(text)\n",
    "    sumVector = np.zeros(dim)\n",
    "\n",
    "    for t in text:\n",
    "        sumVector = np.add(sumVector,model.wv[t])\n",
    "    return np.divide(sumVector,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                Wow... Loved this place\n",
       "1                                      Crust is not good\n",
       "2               Not tasty and the texture was just nasty\n",
       "3      Stopped by during the late May bank holiday of...\n",
       "4      The selection on the menu was great and so wer...\n",
       "                             ...                        \n",
       "995    I think food should have flavor and texture an...\n",
       "996                              Appetite instantly gone\n",
       "997    Overall I was not impressed and would not go back\n",
       "998    The whole experience was underwhelming, and I ...\n",
       "999    Then, as if I hadn't wasted enough of my life ...\n",
       "Name: 0, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    [wow, loved, place]\n",
       "1                                          [crust, good]\n",
       "2                                [tasty, texture, nasty]\n",
       "3      [stopped, late, bank, holiday, rick, steve, re...\n",
       "4                       [selection, menu, great, prices]\n",
       "                             ...                        \n",
       "995              [think, food, flavor, texture, lacking]\n",
       "996                          [appetite, instantly, gone]\n",
       "997                                 [overall, impressed]\n",
       "998    [experience, underwhelming, think, ninja, sush...\n",
       "999    [hadnt, wasted, life, poured, salt, wound, dra...\n",
       "Name: 0, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizedReviews=df[0].apply(lambda x:preprocess(x))\n",
    "tokenizedReviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(np.array(tokenizedReviews),df[1], test_size=0.2,stratify=df[1],random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating vocabulary of unique words from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for t in x_train:\n",
    "    for w in t:\n",
    "        if w not in words:\n",
    "            words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count feature matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainMat = np.zeros((len(x_train),len(words)))\n",
    "for i in range(len(x_train)):\n",
    "    \n",
    "    for w in x_train[i]:\n",
    "        # print(w)\n",
    "        idx = words.index(w)\n",
    "        # print(idx)\n",
    "        TrainMat[i][idx]+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 1599)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainMat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestMat = np.zeros((len(x_test),len(words)))\n",
    "for i in range(len(x_test)):\n",
    "    \n",
    "    for w in x_test[i]:\n",
    "        if w in words:\n",
    "            idx = words.index(w)\n",
    "            TestMat[i][idx]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1599)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TestMat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes with add-1 smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=1)\n",
    "clf.fit(TrainMat,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and F-1 Scores\n",
    "\n",
    "\n",
    "#### Validation: Accuracy = 81%, F1-Score = 0.8173076923076923\n",
    "#### Training: Accuracy = 95.625%, F1-Score = 0.9561952440550688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(TestMat,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95625"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(TrainMat,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8173076923076923"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, clf.predict(TestMat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9561952440550688"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, clf.predict(TrainMat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e309dd32651dd900b6caff0f2fa8696750e584d6f769ca328ef3295da6edd2ea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
